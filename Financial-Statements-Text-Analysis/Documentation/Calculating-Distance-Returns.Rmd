---
title: "TextDistanceAlgo"
author: "Eric He"
date: "August 19, 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library("quanteda")
library("dplyr")
library("purrr")
library("stringr")
library("readtext")
library("reshape2")
library("magrittr")
library("ggplot2")
library("gridExtra")
```

```{r}
masterIndex <- read.csv("masterIndex.csv")
masterIndex$filing %<>% as.character
tickers <- readLines("tickers.txt") # use unique(masterIndex) if we wish to scale this across multiple years, or keep tickers.txt updated
StopWordsList <- readLines("StopWordsList.txt")
hpr <- read.csv("annualReturns.csv", na.strings = "NA")
sections <- c("1", "1A", "3", "4", "7", "8", "9", "9A")
section_names <- c("Business", "Risk Factors", "Legal Proceedings", "Mine Safety Disclosures", "MDA of Financial Conditions", "Financial Statements and Supplementary Data", "Changes on Accounting and Financial Disclosure", "Controls and Procedures")
```

The new section extractor algorithm is much more flexible and robust than the old algorithm. Before, we had an issue where we frequently got the table of contents of a financial statement masquerading as an extra 20 sections. Other, much more rare situations include problematic formatting by the financial statements which would tag normal words as section headings. This problem is solved by choosing the hit with the maximum word count and making it the target section. There are only a few niche cases were a mistagged string is longer than the actual section, and almost no situations where the table of contents is longer than the section itself.

```{r}
# 1 statement, 1 section
section_extractor <- function(statement, section){
  name <- statement$doc_id # needs to be atomic vector
  pattern <- paste0("°Item ", section, "[^\\w|\\d]", ".*?°") # exclude any Item X where X is followed by any unexpected alphanumeric character
  # needs simplify=TRUE because FALSE returns 1-element list of multiple vectors which map() cannot handle. May file issue with stringr
  section_hits <- str_extract_all(statement, pattern, simplify=TRUE) 
  #if section_hits is empty then we need function to skip this one
  if (is_empty(section_hits) == TRUE){
    return("empty")
  }
  word_counts <- map_int(section_hits, ntoken)
  max_hit <- which(word_counts == max(word_counts))
  max_filing <- section_hits[[max_hit[length(max_hit)]]] # select the "filing" with the largest word count. If two hits have the same word count, choose the last one. (Following the idea that the first one is probably ToC; it doesn't really matter which one we pick because we're tossing it out anyway because it definitely doesnt make word count.)
 names(max_filing) <- paste(name, section, sep = "_") 
  return(max_filing)
}

# multiple statements, 1 section. We use logs to discount frequently occurring words. No inverse document frequency is used because it is not useful for comparisons of documents which should be functionally equivalent (idf is used to differentiate documents with entirely different subject matters, since it highlights differences in word choice. In two risk factor sections, it is easy to discount words like "risk" or "dispute", which are still important words to us).

section_dfm <- function(statements_list, section, min_words, tf){
    map(statements_list, section_extractor, section=section) %>%
    map(corpus) %>%
    reduce(`+`) %>%
    dfm(tolower=TRUE, remove=StopWordsList, remove_punct=TRUE) %>% 
    dfm_subset(., rowSums(.) > min_words) %>%
    when(tf==TRUE ~ tf(., scheme="log"), 
         ~ .)
}
# the when statement looks like black magic but it is the functional version of an if-else statement.
# syntax denoted by formula (~) object, LHS of ~ is the condition, RHS is the return.
# If tfidf (the tfidf parameter) == TRUE then return tfidf(., scheme_tf="logave") (the tfidf function)
# Else (no condition) then return . (return the input as the output (do nothing))

# multiple statements, multiple sections, 1 ticker. No reduce() since each filing section needs its own corpus
filing_dfm <- function(sections, filings_list, min_words, tf){
  map(sections, section_dfm, statements_list=filings_list, min_words=min_words, tf=tf)
}

# perform distance analysis on the processed dfm_list. The dist_parser function tries to wrangle with the distObj which textstat_simil returns. It returns a dataframe showing the cosine distance between each pair of filings.
dist_parser <- function(distObj){
  melted_frame <- as.matrix(distObj) %>%
  {. * upper.tri(.)} %>% # lambda function to extract the upper triangular part of b, since the diagonal is the identity distance and dist object is symmetric
  melt(varnames = c("previous_filing", "current_filing"), value.name = "distance") %>% # comparison filing is always filed before current_filing when using upper triangular
  filter(distance != 0) # cut out identity and duplicates. This assumes that no two legitimate documents are completely orthogonal, which I think is reasonable
  melted_frame$previous_filing %<>% str_extract(pattern = ".*?(?=\\.)") # cut out the text name/section
  melted_frame$current_filing %<>% str_extract(pattern = ".*?(?=\\.)") # to allow for easy joining with financial returns
  return(melted_frame)
}

filing_similarity <- function(dfm_list, method){
  map(dfm_list, textstat_simil, method=method) %>%
  map(dist_parser)}

index_filing_filterer <- function(ticker, index){
  filter(index, TICKER == ticker) %>%
    pull(filing) # pull the file name, which in this case is just the filing number
}
index_year_filterer <- function(ticker, index){
  filter(index, TICKER == ticker) %>%
    pull(YEAR)
}

plotter <- function(dfObj, section, nquantiles = 5){
  dfObj %>%
    na.omit %>%
    mutate(quantile = ntile(distance, n = nquantiles)) %>%
    group_by(quantile) %>%
    summarise(average_return = mean(as.numeric(returns)) - 1) %>%
    ggplot(aes(x = quantile, y = average_return)) +
    geom_bar(stat = "identity") +
    theme(axis.title.y=element_blank()) +
    #coord_cartesian(ylim = c(-.2, .3)) +
    xlab(section)
}
```

Do 1 ticker end to end.

```{r}
index_filing_filterer <- function(ticker, index){
  filter(index, TICKER == ticker) %>%
    pull(filing) # pull the file name, which in this case is just the filing number
}
index_year_filterer <- function(ticker, index){
  filter(index, TICKER == ticker) %>%
    pull(YEAR)
}

file_path <- "parsed/"
file_type <- ".txt"

the_ticker <- "AAPL"

file_names <- index_filing_filterer(the_ticker, masterIndex)
file_years <- index_year_filterer(the_ticker, masterIndex)

file_locations <- paste0(file_path, file_names, file_type)

filings_list <- map(file_locations, readtext)

years <- paste0("X", file_years[-1]) # financial return columns start with X bc colnames cannot only be numbers; chop out the first year
returns_df <- filter(hpr, ticker == the_ticker) %>% # calling the_ticker ticker gives rise to namespace issues
  select(years) %>% 
  t() 
colnames(returns_df) <- "returns"
returns_df %<>% cbind(previous_filing = file_names[-length(file_names)], current_filing = file_names[-1], .) %>%
  as.data.frame(stringsAsFactors=FALSE)

similarity_list <- filing_dfm(sections=sections, filings_list=filings_list, min_words=100, tf=TRUE) %>%
  filing_similarity("cosine") # jaccard distance doesnt need any term weightings, although tf=TRUE doesnt make any difference
# similarity_list[map_dbl(similarity_list, nrow) == 0] <- NULL # not needed as we rbind

distance_returns_df2 <- similarity_list %>% # MOVE THE FILTERING HERE, MORE FLEXIBLE THIS WAY
  map(right_join, returns_df)
  #map(~ data_frame(distance=., returns=returns_vector))
```
Do all tickers.

No financial returns data for 2033 of the tickers.

```{r}
tickers %in% hpr$ticker %>% table
```

```{r}
file_path <- "parsed/"
file_type <- ".txt"

distance_returns_calculator <- function(the_ticker){
file_names <- index_filing_filterer(the_ticker, masterIndex)
file_years <- index_year_filterer(the_ticker, masterIndex)

if (length(file_names) <= 1 | length(file_years) <= 1){
  empty_list <- map(rep(NA, times = length(sections)), ~data_frame(previous_filing = ., current_filing = ., distance = ., returns = .))
  print(paste("Only one filing available for ticker", the_ticker))
  return(empty_list)
} # companies with only one year of data cannot be used for a distance analysis. We return a data frame of NAs so that the rbind() can be smooth

years <- paste0("X", file_years[-1]) # chop out the first year
returns_df <- filter(hpr, ticker == the_ticker) %>% # calling the_ticker ticker gives rise to namespace issues
  select(years) %>% 
  t() 
if (is_empty(returns_df) == TRUE){
  empty_list <- map(rep(NA, times = length(sections)), ~data_frame(previous_filing = ., current_filing = ., distance = ., returns = .))
  print(paste("No financial data for ticker", the_ticker))
  return(empty_list)
}

file_locations <- paste0(file_path, file_names, file_type)

filings_list <- map(file_locations, readtext)

colnames(returns_df) <- "returns"
returns_df %<>% cbind(previous_filing = file_names[-length(file_names)], current_filing = file_names[-1], .) %>% # assumes no broken years in the data; broken years can occur if there is no financial filing located one year, or financial data no existerino for a year. Obviously this occurring would be very nonstandard
  as.data.frame(stringsAsFactors = FALSE)

similarity_list <- filing_dfm(sections=sections, filings_list=filings_list, min_words=100, tf=FALSE) %>%
  filing_similarity("jaccard")

distance_returns_df <- similarity_list %>%
  map(right_join, returns_df, by = c("previous_filing", "current_filing"))
print(paste("Successfully mapped distance scores to financial returns for ticker", the_ticker))
return(distance_returns_df)
}

distance_returns_df <- map(tickers, distance_returns_calculator) %>%
  pmap(rbind) # pmap takes the list of lists and rbinds each of the elements within the nested list together. its black magic

save(distance_returns_df, file = "jaccard_distance_returns_df.RData")
  
distance_returns_plot <- distance_returns_df %>%
  map2(sections, ~plotter(dfObj = .x, section = .y)) %>%
  arrangeGrob(grobs = ., ncol = 4, top = "Average Yearly Financial Returns By Jaccard Distance Quantile", left = "Average Yearly Return", bottom = "Filing Section")
  
ggsave(distance_returns_plot, file = "jaccard_distance_returns.png", width = 7, height = 5)
plot(distance_returns_plot)
```
